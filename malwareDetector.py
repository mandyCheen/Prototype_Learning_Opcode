from typing import Any, Tuple
import os
import pandas as pd
from pandas import DataFrame
from numpy import array
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import pickle
import warnings
import torch
from prototypeLearning import prototypical_loss, prototypical_loss_using_proto, contrastive_loss, get_prototypes, nn_prototypical_loss, prototypical_loss_with_cross_entropy
from prototypeLearning import OpcodeEmbedding, PrototypeNet, Autoencoder
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from opcodeSampler import OpcodeSampler


class malwareDetector:
    '''
    This class is used to detect malware samples.
    '''
    def __init__(self, cluster: bool = True, train: bool = True, val: bool = False,
                 cpuArch: str = "crossArch", seed: int = 7, 
                 support_shots = 5, query_shots = 5,
                 support_shots_test = 5, query_shots_test = 15,
                 class_per_iter = None, class_per_iter_test = 5,
                 epochs = 100, iterations = 100,
                 learning_rate = 0.001, dropout_prob = 0.5,
                 lr_scheduler = 20, lr_scheduler_gamma = 0.5,
                 hidden_size = 64, output_size = 64, embedding_model = None,
                 cuda = False, loss = "", cluster_method = "", length = 2000, splitByCpu = False,
                 dataset = "original"):
        # Folder
        self.malwareFolder = "/home/mandy900619/data/Malware202403/"
        self.rawDataset = f"./dataset/raw_csv/malware_{dataset}_{cpuArch}_opcode_dataset.csv"
        self.datasetSplitFolder = f"./dataset/split/{dataset}"
        self.modelFolder = f"./model/{dataset}"
        self.embeddingFolder = f"./embedding/{dataset}"
        self.clusterDataFolder = "./clusterData"
        self.clusterResultFolder = "./clusterResult"
        self.picFolder = "./pic"

        self.cluster = cluster
        self.train = train
        self.val = val
        self.cpuArch = cpuArch
        self.seed = seed
        self.byte_sequence_length = length
        self.dataset = dataset

        self.support_shots = support_shots
        self.query_shots = query_shots
        self.support_shots_test = support_shots_test
        self.query_shots_test = query_shots_test
        self.class_per_iter = class_per_iter
        self.class_per_iter_test = class_per_iter_test
        self.epochs = epochs
        self.iterations = iterations
        self.learning_rate = learning_rate
        self.lr_scheduler = lr_scheduler
        self.lr_scheduler_gamma = lr_scheduler_gamma
        self.dropout_prob = dropout_prob
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.embedding_model = embedding_model
        self.cuda = cuda
        self.loss = loss
        self.cluster_method = cluster_method
        self.splitByCpu = splitByCpu

        self.lossFunc = prototypical_loss
        if loss == "contrastive":
            self.lossFunc = contrastive_loss
        elif loss == "nn_prototypical":
            self.lossFunc = nn_prototypical_loss
        elif loss == "cross_entropy":
            self.lossFunc = prototypical_loss_with_cross_entropy

        splitByCpu = "_splitByCpu" if self.splitByCpu else ""
        val = "_withVal" if self.val else ""
        loss = f"_{self.loss}" if self.loss else ""
        cluster = f"_{self.cluster_method}_cluster" if self.cluster else ""
        embed = f"_{self.embedding_model}" if self.embedding_model else ""

        self.model_name = f"model_{cpuArch}{embed}{cluster}{loss}{splitByCpu}{val}"

    def pretrain_autoencoder(self, model, train_loader, num_epochs, device):
        optimizer = torch.optim.Adam(model.parameters())
        criterion = torch.nn.MSELoss()
        
        model.train()
        for epoch in range(num_epochs):
            total_loss = 0
            for batch in train_loader:
                inputs = batch[0].to(device)
                inputs = inputs.float()
                optimizer.zero_grad()
                _, reconstructions = model(inputs)
                loss = criterion(reconstructions, inputs)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}")


    def trainModel(self, train_X, train_Y, val_X = None, val_Y = None, save_model = True, new_label_mapping = None):
        if not os.path.exists(f"{self.modelFolder}"):
            os.makedirs(f"{self.modelFolder}")
        if self.class_per_iter is None:
            self.class_per_iter = len(np.unique(train_Y))
        
        print(f"Classes per iteration: {self.class_per_iter}")
        print(f"N-shot: {self.support_shots}")
        # dataLoader
        train_tensor = torch.tensor(train_X)
        train_label_tensor = torch.tensor(train_Y)
        trainDataset = TensorDataset(train_tensor, train_label_tensor)
        sampler = OpcodeSampler(labels=train_Y, n_shot=self.support_shots + self.query_shots, classes_per_iter=self.class_per_iter, iterations=self.iterations)
        trianLoader = DataLoader(trainDataset, batch_sampler=sampler)
        if val_X is not None:
            val_tensor = torch.tensor(val_X)
            val_label_tensor = torch.tensor(val_Y)
            valDataset = TensorDataset(val_tensor, val_label_tensor)
            samplerVal = OpcodeSampler(labels=val_Y, n_shot=self.support_shots_test + self.query_shots_test, classes_per_iter=self.class_per_iter_test, iterations=self.iterations)
            valLoader = DataLoader(valDataset, batch_sampler=samplerVal)
        else:
            valLoader = None
        # model
        if self.embedding_model is None:
            embedding_model = OpcodeEmbedding(train_tensor.shape[1], self.hidden_size, self.output_size, self.dropout_prob)
        elif self.embedding_model == "autoencoder":
            autoencoderDataLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)
            print("Pretrain autoencoder...")
            self.autoencoder = Autoencoder(train_tensor.shape[1], self.hidden_size, self.output_size, self.dropout_prob)
            self.pretrain_autoencoder(self.autoencoder, autoencoderDataLoader, 100, 'cuda:0' if torch.cuda.is_available() and self.cuda else 'cpu')
            embedding_model = self.autoencoder.encoder

        model = PrototypeNet(embedding_model)
        optim = torch.optim.Adam(model.parameters(), lr=self.learning_rate)
        # optim = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=self.lr_scheduler, gamma=self.lr_scheduler_gamma)
        
        loss_min = 100
        train_loss = []
        train_acc = []
        val_loss = []
        val_acc = []
        best_acc = 0


        for epoch in range(self.epochs):
            print('=== Epoch: {} ==='.format(epoch))
            model.train()
            # prototypes = dict()
            # prototypesCounts = dict()
            for i, (data, labels) in enumerate(trianLoader):
                optim.zero_grad()
                data = data.squeeze(1)
                data = data.float()
                embeddings = model(data)
                loss, acc, proto, protoID, _, _, _= self.lossFunc(embeddings, target=labels, n_support=self.support_shots)
                # if self.cluster and valLoader is not None:
                #     for i in range(proto.shape[0]):
                #         if prototypesCounts.get(protoID[i].item()) is None:
                #             prototypesCounts[protoID[i].item()] = 1
                #             prototypes[protoID[i].item()] = proto[i]
                #         else:
                #             prototypesCounts[protoID[i].item()] += 1
                #             prototypes[protoID[i].item()] += proto[i]
                loss.backward()
                optim.step()   
                train_loss.append(loss.item())
                train_acc.append(acc.item()) 
            lr_scheduler.step()
            # if self.cluster and valLoader is not None:
            #     prototypes = {k: v / prototypesCounts[k] for k, v in prototypes.items()}
            #     prototypes = dict(sorted(prototypes.items()))
            avg_loss = np.mean(train_loss[-(self.iterations):])
            avg_acc = np.mean(train_acc[-(self.iterations):])
            postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)
            print('Avg Train Loss: {}, Avg Train Acc: {}{}'.format(avg_loss, avg_acc, postfix))
            if valLoader is not None: # NEED TO MODIFY
                model.eval()
                for i, (data, labels) in enumerate(valLoader):
                    data = data.squeeze(1)
                    data = data.float()
                    model_output = model(data)
                    # if self.cluster:
                    #     loss, acc, _= prototypical_loss_using_proto(model_output, target=labels, prototypes=prototypes, label_mapping=new_label_mapping)
                    # else:
                    loss, acc, _, _, _, _, _= self.lossFunc(model_output, target=labels, n_support=self.support_shots)
                    val_loss.append(loss.item())
                    val_acc.append(acc.item())
                avg_loss = np.mean(val_loss[-(self.iterations):])
                avg_acc = np.mean(val_acc[-(self.iterations):])
                postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)
                print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_loss, avg_acc, postfix))

            if save_model and avg_acc > best_acc:
                torch.save(model.state_dict(), f"{self.modelFolder}/{self.model_name}.pt")
                print(f"Model saved at {self.modelFolder}/{self.model_name}.pt")
                best_acc = avg_acc
                best_state = model.state_dict()
                
            if save_model:
                torch.save(model.state_dict(), f"{self.modelFolder}/{self.model_name}_check_point.pt")

        return best_state, best_acc, train_loss, train_acc, val_loss, val_acc


    def testModel(self, test_X, test_Y, new_label_mapping = None, modelPath = None, class_per_iter = None):
        '''
        Test the model trained with the prototypical learning algorithm
        '''
        device = 'cuda:0' if torch.cuda.is_available() and self.cuda else 'cpu'
        avg_acc = list()
        if class_per_iter is None:
            class_per_iter = self.class_per_iter_test
        data_tensor = torch.tensor(test_X)
        label_tensor = torch.tensor(test_Y)
        test_dataset = TensorDataset(data_tensor, label_tensor)
        sampler = OpcodeSampler(labels=test_Y, n_shot=self.support_shots_test + self.query_shots_test, classes_per_iter=class_per_iter, iterations=self.iterations)
        testLoader = DataLoader(test_dataset, batch_sampler=sampler)
        # model
        if self.embedding_model is None:
            embedding_model = OpcodeEmbedding(data_tensor.shape[1], self.hidden_size, self.output_size, self.dropout_prob)
        elif self.embedding_model == "autoencoder":
            embedding_model = self.autoencoder.encoder
        model = PrototypeNet(embedding_model)
        if modelPath is None:
            print(f"Load model from {self.modelFolder}/{self.model_name}.pt")
            model.load_state_dict(torch.load(f"{self.modelFolder}/{self.model_name}.pt"))
        elif modelPath == "NULL":
            print("No model loaded")
            pass
        else:
            print(f"Load model from {modelPath}")
            model.load_state_dict(torch.load(modelPath))
        model.to(device)
        model.eval()
        print(f"Model : {model}")

        if self.cluster:
            prototypes = get_prototypes(model, testLoader, self.support_shots)
        else:
            prototypes = None
        
        for epoch in range(10):
            for i, (x, y) in enumerate(testLoader):
                x, y = x.to(device), y.to(device)
                x = x.squeeze(1)
                x = x.float()
                x, y = x.to(device), y.to(device)
                model_output = model(x)
                # if prototypes is None:
                _, acc, _, _, _, _, _= self.lossFunc(model_output, target=y, n_support=self.support_shots_test)
                # else:
                #     acc, _= prototypical_loss_using_proto(model_output, target=y,prototypes=prototypes, label_mapping=new_label_mapping)
                avg_acc.append(acc.item())
        avg_acc = np.mean(avg_acc)
        print('Test Acc: {}'.format(avg_acc))
        return avg_acc
    
    def get_model(self, modelName=None, featureDim=1000):
        if self.embedding_model is None:
            embedding_model = OpcodeEmbedding(featureDim, self.hidden_size, self.output_size, self.dropout_prob)
        elif self.embedding_model == "autoencoder":
            embedding_model = self.autoencoder.encoder
        model = PrototypeNet(embedding_model)
        if modelName is None:
            model.load_state_dict(torch.load(f"{self.modelFolder}/{self.model_name}.pt"))
        else:
            model.load_state_dict(torch.load(f"{self.modelFolder}/{modelName}.pt"))
        return model
